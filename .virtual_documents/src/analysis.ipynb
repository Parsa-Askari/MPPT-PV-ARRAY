%load_ext autoreload


%autoreload 2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from os.path import join
import csv
from tqdm.auto import tqdm
import re
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler , PolynomialFeatures
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error , r2_score
from xgboost import XGBRegressor
from Feature_engineering.FE import *
from Feature_engineering.utils import *
from Feature_engineering.ProjectPaths import *





os.makedirs(output_folder_paths,exist_ok=True)
os.makedirs(golden_iv_paths,exist_ok=True)
os.makedirs(eugene_iv_paths,exist_ok=True)
os.makedirs(cocoa_iv_paths,exist_ok=True)


cocoa_datasets=[join(folder_path,"Cocoa",paths) for paths in os.listdir(join(folder_path,"Cocoa")) if paths.split(".")[-1]=="csv"]
eugene_datasets=[join(folder_path,"Eugene",paths) for paths in os.listdir(join(folder_path,"Eugene")) if paths.split(".")[-1]=="csv"]
golden_datasets=[join(folder_path,"Golden",paths) for paths in os.listdir(join(folder_path,"Golden")) if paths.split(".")[-1]=="csv"]


golden_df=pretty_csv(golden_datasets,golden_iv_paths)
eugene_df=pretty_csv(eugene_datasets,eugene_iv_paths)
cocoa_df=pretty_csv(cocoa_datasets,cocoa_iv_paths)


golden_df.rename(columns=rename_dict,inplace=True)
eugene_df.rename(columns=rename_dict,inplace=True)
cocoa_df.rename(columns=rename_dict,inplace=True)

golden_df.to_csv(join(output_folder_paths,"golden.csv"),index=False)
eugene_df.to_csv(join(output_folder_paths,"eugene.csv"),index=False)
cocoa_df.to_csv(join(output_folder_paths,"cocoa.csv"),index=False)


golden_df=pd.read_csv(join(output_folder_paths,"golden.csv"))
eugene_df=pd.read_csv(join(output_folder_paths,"eugene.csv"))
cocoa_df=pd.read_csv(join(output_folder_paths,"cocoa.csv"))


main_df=pd.concat([golden_df,eugene_df,cocoa_df],axis=0)
# main_df=golden_df


main_df.columns


main_df["filename"]=main_df["filename"].apply(lambda x : x.split("_")[-1])


main_df.groupby("filename")["filename"].value_counts()


fig,ax=plt.subplots(1,2,figsize=(20,10))

missing_data=main_df[(main_df["DNI_unc"]<0) | (main_df["GHI_unc"]<0)]["filename"].value_counts()
missing_data=missing_data.sort_index()

cmap   = plt.get_cmap("tab20b")  
colors = cmap(np.linspace(0, 1, len(missing_data)))

wedges,_,_=ax[0].pie(missing_data,autopct="%1.1f%%",colors=colors)
ax[0].legend(
    wedges,
    missing_data.index,
    title="filename",
    loc="center left",
    bbox_to_anchor=(1, 0.5),
)

data_counts=main_df.groupby("filename")["filename"].value_counts()
data_counts=data_counts.sort_index()
wedges,_,_=ax[1].pie(data_counts,autopct="%1.1f%%",colors=colors)
ax[1].legend(
    wedges,
    missing_data.index,
    title="filename",
    loc="center left",
    bbox_to_anchor=(1, 0.5)
)


ax[0].set_title("Missing Data", fontsize=20)
ax[1].set_title("Data Counts", fontsize=20)
plt.tight_layout()


print(len(main_df))
main_df=remove_invalid_rows(main_df)
print(len(main_df))


main_df.describe().min()


main_df.dtypes


columns=main_df.columns
unc_cols = [col for col in main_df.columns if (col.endswith('_unc') or col.endswith("_std"))]
lickage_features=["Isc (A)","TimeStamp","FF (%FF)","Voc (V)"]+unc_cols


to_drop=lickage_features+["filename","I-V paths"]


corr=(main_df.drop(columns=to_drop)).corr()
uncorrlated_cols=[]
for col in corr.columns : 
    if(corr[col].sum()==0):
        uncorrlated_cols.append(col)
print("removing these columns : ",uncorrlated_cols)
main_df.drop(columns=uncorrlated_cols,inplace=True)
columns=main_df.columns


corr=(main_df.drop(columns=to_drop)).corr(method="spearman")
fig,ax=plt.subplots(1,1,figsize=(15,5))
sns.heatmap(corr[["Imp (A)","Vmp (V)","Pmp (W)"]].T,ax=ax,cmap="jet",annot=True,fmt=".2f")#mask=np.tri(corr.shape[0]).T
# corr["Imp (A)"].sort_values()


fig,ax=plt.subplots(1,2,figsize=(25,8))
d=main_df.drop(columns=lickage_features+["I-V paths"])
c=d.groupby("filename").corr(method="pearson")["Vmp (V)"]
corr_matrix = c.unstack(level=1)
sns.heatmap(corr_matrix,ax=ax[0],cmap="jet",annot=True,fmt=".2f")

c=d.groupby("filename").corr(method="spearman")["Vmp (V)"]
corr_matrix = c.unstack(level=1)
sns.heatmap(corr_matrix,ax=ax[1],cmap="jet",annot=True,fmt=".2f")

ax[0].set_title("Correlations For Vmp (V) pearson")
ax[1].set_title("Correlations For Vmp (V) spearman")


fig,ax=plt.subplots(1,1,figsize=(15,8))
d=main_df.drop(columns=lickage_features+["I-V paths"])
c=d.groupby("filename").corr()["Imp (A)"]
corr_matrix = c.unstack(level=1)
sns.heatmap(corr_matrix,ax=ax,cmap="jet",annot=True,fmt=".2f")
ax.set_title("Correlations For Imp (A)")


fig,ax=plt.subplots(1,1,figsize=(15,8))
d=main_df.drop(columns=lickage_features+["I-V paths"])
c=d.groupby("filename").corr()["Pmp (W)"]
corr_matrix = c.unstack(level=1)
sns.heatmap(corr_matrix,ax=ax,cmap="jet",annot=True,fmt=".2f")
ax.set_title("Correlations For Pmp (W)")


corr["Pmp (W)"].sort_values()


columns


sites=main_df["filename"].unique()


X=main_df.drop(columns=["Imp (A)","Pmp (W)","Vmp (V)"])
Y=main_df[["Imp (A)","Pmp (W)","Vmp (V)","filename"]]
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42,stratify=X["filename"])


features=["POA_CMP22","Dry_Temp"]
labels=["Imp (A)","Pmp (W)","Vmp (V)"]
n_row=len(labels)
n_col=len(features)
subplot_size=(8,8)
fig,axes=plt.subplots(n_row,n_col,figsize=(n_col*subplot_size[1],n_row*subplot_size[0]))
for i in range(n_row):
    for j in range(n_col):
        ax=axes[i*j] if (n_col==1 or n_row == 1) else axes[i,j]
        x_col=features[j]
        y_col=labels[i]
        sns.scatterplot(
            x=X[x_col],
            y=Y[y_col],
            hue=X["filename"],
            palette="tab20",
            ax=ax,
            legend="full",
            alpha=0.8
        )
# plt.show()
plt.savefig("full_scatter.png", bbox_inches='tight')


Y[["Imp (A)","Vmp (V)"]].mean()





n_sample=6
samples=main_df.sample(random_state=42,n=n_sample)
sampled_paths=samples["I-V paths"].values
samped_imps=samples["Imp (A)"].values
samped_vmps=samples["Vmp (V)"].values
samped_pmps=samples["Pmp (W)"].values


n_row=2
n_col=3
fig,axes=plt.subplots(n_row,n_col,figsize=(5*n_col,5*n_row))
scale=10
for i in range(n_row):
    for j in range(n_col):
        index=((i+1)*(j+1)) - 1
        iv_points=np.load(sampled_paths[index])
        imp=samped_imps[index]
        vmp=samped_vmps[index]/scale
        pmp=samped_pmps[index]/scale
        
        i_array=iv_points[:,0]
        v_array=iv_points[:,1]/scale
        p_array=(i_array*v_array)
        axes[i,j].plot(v_array,i_array,color="blue",label="V-I curve")
        axes[i,j].plot(v_array,p_array,color="red",label="V-P curve")
        axes[i,j].axhline(imp, color='k', ls='--')
        axes[i,j].axhline(pmp, color='r', ls='--')
        axes[i,j].axvline(vmp, color='g', ls='--')
        
        axes[i,j].set_xlabel("V")
        axes[i,j].set_ylabel("I|P")
        axes[i,j].text(0, imp, r'$I_{MPP}$',   va='bottom', ha='left')
        axes[i,j].text(0, pmp, r'$P_{MPP}$',   va='bottom', ha='left' , color='r')
        axes[i,j].text(vmp, -0.04, r'$V_{MPP}$', color='g',va='bottom', ha='left')
        axes[i,j].legend()









